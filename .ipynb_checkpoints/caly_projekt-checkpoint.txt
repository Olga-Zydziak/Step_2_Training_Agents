--- FILE: __init__.py ---




--- FILE: config.py ---

import os
import logging
from enum import Enum
from google.cloud import secretmanager
import langchain
from langchain.cache import SQLiteCache





def get_secret(project_id: str, secret_id: str, version_id: str = "latest") -> str:
    """Pobiera wartość sekretu z Google Secret Manager."""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})
   
    return response.payload.data.decode("UTF-8")


class ApiType(Enum):
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    OPENAI = "openai"
    def __str__(self):
        return self.value


LOCATION="us-central1"
PROJECT_ID="dark-data-discovery"

#---------AGENTS--------:
MAIN_AGENT="gemini-2.5-pro"
API_TYPE_GEMINI=str(ApiType.GOOGLE)

CRITIC_MODEL="claude-3-7-sonnet-20250219"
CODE_MODEL="claude-sonnet-4-20250514"
QUICK_SMART_MODEL="gemini-2.5-flash"

GPT_MODEL = "gpt-5" # Używamy gpt-4o jako odpowiednika "gpt-5"
API_TYPE_OPENAI = str(ApiType.OPENAI)

API_TYPE_SONNET = str(ApiType.ANTHROPIC)

LANGCHAIN_API_KEY = get_secret(PROJECT_ID,"LANGCHAIN_API_KEY")
ANTHROPIC_API_KEY=get_secret(PROJECT_ID,"ANTHROPIC_API_KEY")
TAVILY_API_KEY = get_secret(PROJECT_ID,"TAVILY_API_KEY")
OPENAI_API_KEY = get_secret(PROJECT_ID, "OPENAI_API_KEY")

MEMORY_ENGINE_DISPLAY_NAME="memory-gamma-way"

INPUT_FILE_PATH = "gs://super_model/data/structural_data/synthetic_fraud_dataset.csv"

MAX_CORRECTION_ATTEMPTS=5



os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "Projekt Multi-Agent-System Dynamic-graphs"
os.environ["ANTHROPIC_API_KEY"] =ANTHROPIC_API_KEY
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
#---cache-------
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")




#FUNKCJA KONFIGURACYJNA AGENTOW AUTOGEN
def basic_config_agent(agent_name:str, api_type:str, location:str=None, project_id:str=None, api_key:str=None):
    try:
        configuration = {"model": agent_name}
        configuration.update({"api_type": api_type})
        if api_key: configuration["api_key"] = api_key
        if project_id: configuration["project_id"] = project_id
        if location: configuration["location"] = location

        logging.info(f"Model configuration: {configuration}")
        return [configuration]

    except Exception as e:
        logging.error(f"Failed to initialize Vertex AI or configure LLM: {e}")
        print(f"Error: Failed to initialize Vertex AI or configure LLM. Please check your project ID, region, and permissions. Details: {e}")
        exit()



--- FILE: main.ipynb ---

!gcloud config get-value account
# --- Koniec komórki ---
import pandas as pd
import config
from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET,QUICK_SMART_MODEL,ANTHROPIC_API_KEY,basic_config_agent
from dynamic_graph.outputsModels import *
from dynamic_graph.main_graph_functions import run_collaborative_planning
from prompts import PromptEngine
from dynamic_graph.state import WorkflowState
from dynamic_graph.main_graph_functions import build_and_run_graph
# --- Koniec komórki ---
# # --- Konfiguracja czatu grupowego ---
# main_agent_configuration={"cache_seed": 42,"seed": 42,"temperature": 0.0,
#                         "config_list": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}
# critic_agent_configuration ={"cache_seed": 42,"seed": 42,"temperature": 0.0,
#                         "config_list": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}
router_agent_configuration = {"cache_seed": 42,"seed": 42,"temperature": 0.0,
                              "config_list": basic_config_agent(agent_name=QUICK_SMART_MODEL, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}

# --- Koniec komórki ---
if __name__ == '__main__':
    # Konfiguracja dla lekkiego i szybkiego modelu Rutera
    
    
    # Wysokopoziomowa misja od użytkownika
    mission= """
    Zaprojektuj ODPORNY NA BŁĘDY, sekwencyjny przepływ pracy do weryfikacji modelu przyczynowego.
    Kroki to: load_data, discover_causality i validate_model.
    Węzeł 'discover_causality' jest ryzykowny i po nim musi nastąpić sprawdzenie,
    czy operacja się powiodła. W razie błędu, proces musi zostać skierowany do uniwersalnego debuggera,
    a następnie spróbować ponownie wykonać krok 'discover_causality'.
    """
    
    # Uruchom całą fazę planowania jedną komendą
    final_plan = run_collaborative_planning(mission, router_agent_configuration)
    
    if final_plan:
        print("\n" + "="*50)
        print("### OTRZYMANY PLAN GOTOWY DO WYKONANIA ###")
        print(final_plan.model_dump_json(indent=2, by_alias=True))
        
        initial_state = {
            "mission":mission,
            "input_path":INPUT_FILE_PATH, # Używamy ścieżki z pliku konfiguracyjnego
            "dataframe":pd.DataFrame(), # Pusta ramka danych na start
            "causal_graph":None,
            "validation_report":{},
            "final_summary":"",
            "error":""
        }
        
        # Uruchamiamy fabrykę, aby zbudowała i uruchomiła graf na podstawie planu
        build_and_run_graph(
            plan=final_plan,
            state_schema=WorkflowState,
            initial_state=initial_state
        )
        
    else:
        print("\nNie udało się wypracować planu. Sprawdź logi w `reports/`.")

    
    
# --- Koniec komórki ---

# --- Koniec komórki ---



--- FILE: prompts.py ---

import json
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Type
from dynamic_graph.outputsModels import *
# =================================================================================
# Sekcja 1: DYREKTYWY SYSTEMOWE (PERSONY NADRZĘDNE)
# Skopiowane z Twojego pliku prompts_beta (1).py
# =================================================================================

SYSTEM_PROMPT_ANALYST = """
# CORE DIRECTIVE: STRATEGIC AI ANALYST "ORACLE"
Jesteś "Oracle", elitarnym analitykiem AI specjalizującym się w strategii, analizie i krytycznym myśleniu. Twoim celem jest przetwarzanie złożonych informacji, podejmowanie trafnych decyzji i komunikowanie wniosków z absolutną precyzją.

## CORE PRINCIPLES (NON-NEGOTIABLE)
1.  **Structured Thinking:** Zanim sformułujesz finalną odpowiedź, rozpisz swój proces myślowy krok po kroku w polu `thought_process`.
2.  **Evidence-Based Reasoning:** Twoje decyzje i oceny muszą być oparte wyłącznie na dostarczonych danych (`<CONTEXT>`).
3.  **Adherence to Format:** Ściśle przestrzegaj wymaganego formatu wyjściowego. Od tego zależy stabilność całego systemu.
"""



SPEC_CRITIC_AEGIS = """
## SPECIALIZATION: AEGIS, THE UNYIELDING GUARDIAN OF QUALITY
Twoją wyspecjalizowaną funkcją jest "Aegis" – tarcza chroniąca system przed błędami. Twoim celem jest aktywne wzmacnianie i hartowanie każdego planu poprzez precyzyjną, konstruktywną analizę.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)
1.  **Deep Analysis vs. Superficial Validation:** Nigdy nie akceptuj planu tylko dlatego, że "spełnia minimalne wymagania". Zawsze szukaj ukrytych wad i potencjalnych ryzyk.
2.  **Absolute Adherence to Format:** Twoja odpowiedź ZAWSZE musi być poprawnym obiektem JSON. Musisz także bezwzględnie przestrzegać ZŁOTEJ ZASADY ZAKOŃCZENIA PRACY.

### STEP-BY-STEP THOUGHT PROCESS
1.  **Formal Verification:** Sprawdzenie poprawności składni JSON i zgodności ze schematem.
2.  **Logical & Mission-Based Analysis:** Weryfikacja zgodności z celem z `<MISSION>`.
3.  **Architectural Assessment (Quality Heuristics):** Ocena planu pod kątem prostoty, odporności na błędy, efektywności i czytelności.
4.  **Verdict Formulation:** Podjęcie ostatecznej decyzji i wybór jednego z dwóch trybów działania.

### TWO MODES OF OPERATION
#### Mode 1: FLAW DETECTED (CORRECTION NEEDED)
Jeśli plan ma wadę, opisz ją i zasugeruj rozwiązanie, używając do tego obiektu JSON zawierającego Twoje przemyślenia.

#### Mode 2: FINAL PLAN (APPROVAL)
Jeśli plan jest bezbłędny, Twoja odpowiedź MUSI być **DOKŁADNĄ, NIEZMIENIONĄ KOPIĄ** obiektu JSON, który otrzymałeś od planisty. Musi on zawierać wszystkie pola: `thought_process`, `entry_point`, `nodes` i `edges`. Nie dodawaj od siebie żadnych dodatkowych pól, takich jak `verdict`.

### THE GOLDEN RULE OF TERMINATION
**JEŚLI I TYLKO JEŚLI ZATWIERDZASZ PLAN (Tryb 2), MUSISZ:**
1.  Wkleić **PEŁNY, NIENARUSZONY** obiekt JSON z planem.
2.  **POZA** tym obiektem JSON, na samym końcu wiadomości, dodać kluczową frazę: `PLAN_ZATWIERDZONY`
"""

SPEC_ROUTER = """
## SPECIALIZATION: TEAM COMPOSITION STRATEGIST
Twoją specjalizacją jest strategiczna analiza wymagań misji w celu skomponowania optymalnego zespołu agentów-ekspertów. Działasz jak dyrektor castingu, dobierając odpowiednie talenty do zadania. Twoja decyzja musi być oparta wyłącznie na opisie misji i charakterystyce dostępnych agentów.
"""


SPEC_ARCHITECT = """
## SPECIALIZATION: WORKFLOW ARCHITECT
Twoją specjalizacją jest tłumaczenie wysokopoziomowych misji na precyzyjne, maszynowo-czytelne plany przepływu pracy (workflow) w formacie JSON. Jesteś głównym inżynierem systemu.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)

1.  **Modularity over Monoliths:** Projektuj przepływy jako serię małych, wyspecjalizowanych kroków (węzłów), które można łatwo zrozumieć i testować.
2.  **Plan for Failure:** Zawsze zastanów się, co może pójść nie tak. Nawet jeśli misja wymaga prostego przepływu, w procesie myślowym odnotuj potencjalne punkty awarii.
3.  **Clarity and Readability:** Nazwy węzłów i struktura grafu muszą być intuicyjne i łatwe do zrozumienia dla innego inżyniera.
4.  **Explainability First:** Twój plan musi być w pełni transparentny. Jako OSTATNI krok w każdym przepływie pracy, ZAWSZE dodawaj węzeł `generate_explainability_report`, aby udokumentować proces decyzyjny.
"""


SPEC_CAUSAL_EXPERT = """
## SPECIALIZATION: CAUSAL MODELING ARCHITECT
Twoim zadaniem jest projektowanie wysokopoziomowych planów przepływu pracy (`WorkflowPlan`) dla modeli przyczynowo-skutkowych. Koncentrujesz się na logicznej sekwencji kroków, a nie na szczegółach implementacji.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)
1.  **Logic First:** Skup się na poprawnym uporządkowaniu głównych etapów (np. `load_data` -> `discover_causality` -> `validate_model`).
2.  **Simplicity:** Twój plan powinien być tak prosty i przejrzysty, jak to tylko możliwe. Unikaj zbędnych, skomplikowanych ścieżek, jeśli misja tego nie wymaga.
3.  **Clarity:** Każdy węzeł i krawędź w Twoim planie musi mieć jasny i zrozumiały cel.
"""

SPEC_DATA_ANALYST = """
## SPECIALIZATION: DATA PREPARATION PLANNER
Twoim zadaniem jest tworzenie szczegółowych, technicznych planów przepływu pracy (`WorkflowPlan`) dotyczących czyszczenia, transformacji i walidacji danych.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)
1.  **Data-Driven Decisions:** Każdy krok w Twoim planie musi wynikać bezpośrednio z analizy schematu danych (np. nazw i typów kolumn).
2.  **Granularity:** Dziel złożone operacje na mniejsze, atomowe kroki. Zamiast jednego węzła "przetwórz dane", stwórz osobne węzły dla "imputacji braków", "korekty typów" i "inżynierii cech".
3.  **Justification:** W polu `thought_process` krótko uzasadnij, dlaczego dany krok jest potrzebny (np. "Kolumna 'X' ma 30% braków, dlatego dodaję węzeł imputacji medianą").
"""


# =================================================================================
# Sekcja 3: SPECJALIZACJE DLA AGENTÓW-ROBOTNIKÓW (LANGGRAPH NODES)
# =================================================================================

SPEC_CAUSAL_DISCOVERER = """
## SPECIALIZATION: CAUSAL DISCOVERY SCIENTIST
Twoim jedynym zadaniem jest analiza dostarczonego zbioru danych i odkrycie w nim fundamentalnych zależności przyczynowo-skutkowych. Musisz zaproponować strukturę grafu, która najlepiej oddaje te relacje.
"""

SPEC_MODEL_VALIDATOR = """
## SPECIALIZATION: CRITICAL MODEL AUDITOR
Twoim jedynym zadaniem jest rygorystyczna ocena przedstawionego modelu przyczynowego. Skup się na jego logicznej spójności i potencjalnych słabościach. Twoja walidacja musi być obiektywna i oparta na danych.
"""


SPEC_SELF_HEALING_CAPABILITY = """
## ADDITIONAL CAPABILITY: SELF-HEALING VIA ONLINE RESEARCH
MASZ NOWĄ ZDOLNOŚĆ: Potrafisz używać narzędzia 'search_tool' do wyszukiwania w internecie.
Jeśli Twój kod analityczny zwróci błąd ('AttributeError', 'ValueError', 'TypeError'),
użyj tego narzędzia, aby znaleźć poprawny sposób użycia danej biblioteki w jej
oficjalnej dokumentacji. Następnie popraw swój kod i spróbuj ponownie.
"""

SPEC_ARCHITECT_GPT = """
## SPECIALIZATION: ADVANCED WORKFLOW ARCHITECT (GPT-Powered)
Twoją specjalizacją jest kreatywne i innowacyjne projektowanie złożonych przepływów pracy (workflow) w formacie JSON. Wykorzystujesz zaawansowane możliwości rozumowania, aby tworzyć solidne i efektywne plany.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)
1.  **Think End-to-End:** Zawsze projektuj kompletne przepływy, od załadowania danych aż po finalny raport lub rezultat.
2.  **Anticipate Failure:** Aktywnie szukaj potencjalnych punktów awarii w planie i projektuj ścieżki naprawcze z użyciem dostępnych narzędzi, takich jak `universal_debugger`.
3.  **Optimize for Clarity:** Twój plan musi być nie tylko funkcjonalny, ale również łatwy do zrozumienia dla człowieka.
4.  **Resilience by Design (NAJWAŻNIEJSZA ZASADA):** Twój plan MUSI być odporny na błędy. Przeanalizuj listę dostępnych narzędzi. Jeśli zidentyfikujesz operacje, które są ryzykowne (np. te z adnotacją 'może zakończyć się błędem'), musisz samodzielnie zaprojektować architekturę, która obsłuży potencjalną awarię. Wykorzystaj dostępne narzędzia warunkowe i naprawcze, aby stworzyć pętlę, która w razie błędu podejmie próbę naprawy, a dopiero potem kontynuuje główny przepływ pracy.
5.  **Clarity and Readability:** Nazwy węzłów i struktura grafu muszą być intuicyjne i łatwe do zrozumienia dla innego inżyniera.
6.  **Explainability First:** Twój plan musi być w pełni transparentny. 
"""

class PromptConfig(BaseModel):
    """Generyczna struktura do konfigurowania dowolnego promptu."""
    system_prompt: str
    task_description: str
    context: Dict[str, Any] = Field(default_factory=dict)
    output_schema: Type[BaseModel]

class PromptEngine:
    """Centralny silnik do generowania promptów."""

    @staticmethod
    def build(config: PromptConfig) -> str:
        """Składa finalny, kompletny prompt jako string."""
        prompt = [config.system_prompt]
        prompt.append(f"## TASK\n{config.task_description}")
        if config.context:
            context_str = "\n".join(f"<{key.upper()}>\n{value}\n</{key.upper()}>" for key, value in config.context.items() if value)
            prompt.append(f"## CONTEXT\n{context_str}")
        
        schema_json = json.dumps(config.output_schema.model_json_schema(), indent=2)
        output_format_str = f"Twoja odpowiedź MUSI być poprawnym obiektem JSON zgodnym z poniższym schematem. Nie dodawaj żadnych innych treści poza obiektem JSON.\n```json\n{schema_json}\n```"
        prompt.append(f"## OUTPUT FORMAT\n{output_format_str}")
        return "\n\n".join(prompt)

    # --- Metody do tworzenia gotowych konfiguracji dla Fabryki Agentów ---

    
    
    @staticmethod
    def for_critic() -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Krytyka 'Aegis'."""

        # 1. Budujemy kompletny system_prompt dla Krytyka
        system_prompt_for_critic = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_CRITIC_AEGIS}"

        # 2. "Pakujemy" wszystko do obiektu PromptConfig
        return PromptConfig(
            system_prompt=system_prompt_for_critic,
            task_description="",  # Zadanie jest wbudowane w system_prompt
            context={},            # Kontekst to historia czatu, nic więcej nie potrzeba
            output_schema=WorkflowPlan  # Schemat jest potrzebny do walidacji
        )

    
    @staticmethod
    def for_router(mission: str, agent_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Rutera."""

        # Budujemy kompletny system_prompt, łącząc fundament ze specjalizacją
        system_prompt_for_router = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_ROUTER}"

        # Opis zadania, które agent ma wykonać
        task_description = "Na podstawie poniższej misji i listy dostępnych ekspertów, dobierz optymalny zespół składający się z 1-2 planistów oraz 1 krytyka."

        # Kontekst jest dynamiczny i niezbędny do wykonania zadania
        context = {
            "mission": mission,
            "available_experts": agent_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt_for_router,
            task_description=task_description,
            context=context,
            output_schema=AgentSelection  # Agent ma zwrócić obiekt zgodny z tym schematem
        )
    
    
    
    @staticmethod
    def for_architect(mission: str, node_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Architekta."""

        # Budujemy kompletny system_prompt
        system_prompt_for_architect = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_ARCHITECT_GPT}"

        # Opis zadania do wykonania
        task_description = "Zaprojektuj kompletny i odporny na błędy przepływ pracy (workflow) w formacie JSON, który realizuje zadaną misję, korzystając z dostępnych narzędzi."

        # Kontekst jest niezbędny do zaprojektowania planu
        context = {
            "mission": mission,
            "available_tools": node_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt_for_architect,
            task_description=task_description,
            context=context,
            output_schema=WorkflowPlan # Agent musi zwrócić obiekt `WorkflowPlan`
        )
    @staticmethod
    def for_causal_expert(mission: str, node_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Eksperta od Modeli Przyczynowych."""

        # Łączymy fundament "Oracle" ze specjalizacją
        system_prompt = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_CAUSAL_EXPERT}"

        # Opis zadania, które agent ma wykonać
        task_description = "Zaprojektuj wysokopoziomowy, logiczny plan przepływu pracy (`WorkflowPlan`) dla modelu przyczynowo-skutkowego, opierając się na misji i dostępnych narzędziach."

        # Kontekst, którego potrzebuje do wykonania zadania
        context = {
            "mission": mission,
            "available_tools": node_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt,
            task_description=task_description,
            context=context,
            output_schema=WorkflowPlan  # Oczekiwany format wyjściowy to plan grafu
        )
    
    
    @staticmethod
    def for_data_analyst(mission: str, node_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Analityka Danych."""

        # Łączymy fundament "Oracle" ze specjalizacją
        system_prompt = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_DATA_ANALYST}"

        # Opis zadania, które agent ma wykonać
        task_description = "Zaprojektuj szczegółowy, techniczny plan przepływu pracy (`WorkflowPlan`) dotyczący czyszczenia i walidacji danych, opierając się na misji i dostępnych narzędziach."

        # Kontekst, którego potrzebuje do wykonania zadania
        context = {
            "mission": mission,
            "available_tools": node_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt,
            task_description=task_description,
            context=context,
            output_schema=WorkflowPlan  # Ten agent również ma za zadanie stworzyć plan grafu
        )


--- FILE: tools/__init__.py ---




--- FILE: tools/search_tools.py ---

from langchain_community.tools.tavily_search import TavilySearchResults

# Definiujemy narzędzie, ograniczając je do 3 wyników, aby było zwięzłe.
# LangChain automatycznie użyje zmiennej środowiskowej TAVILY_API_KEY.
search_tool = TavilySearchResults(max_results=3)


--- FILE: dynamic_graph/main_graph_functions.py ---

import json
import os
import io
import autogen
from langgraph.graph import StateGraph, END 
from typing import List, Dict, Any, Optional, Type
from .router import get_structured_response,select_team
from agents.agents_library import DISCUSSION_AGENT_LIBRARY,main_agent_configuration
from agents.agents import CausalExpertAgent, DataScientistAgent
from .outputsModels import WorkflowPlan
from .nodes import NODE_LIBRARY
from prompts import *
import sys
from contextlib import redirect_stdout, redirect_stderr
LOGS_DIR = "reports"
LOG_FILE_PATH = os.path.join(LOGS_DIR, "planning_brainstorm.log")


PROMPT_FACTORY_MAP = {
    CausalExpertAgent: PromptEngine.for_causal_expert,
    DataScientistAgent: PromptEngine.for_data_analyst,
    # Można tu dodawać kolejne typy planerów w przyszłości
}


def run_collaborative_planning(mission: str, router_llm_config: Dict) -> Optional[WorkflowPlan]:
    """
    Orkiestruje całym procesem autonomicznego planowania.

    Jako "reżyser" fazy projektowej, ta funkcja zarządza całym cyklem:
    1. Dynamicznie dobiera zespół agentów-ekspertów za pomocą inteligentnego Rutera.
    2. Konfiguruje ustrukturyzowaną "burzę mózgów" z jasno określonymi zasadami.
    3. Inicjuje dyskusję, przekazując agentom precyzyjnie zbudowany prompt.
    4. Po zakończeniu dyskusji, waliduje i wyodrębnia jej finalny produkt - obiekt WorkflowPlan.

    Args:
        mission: Wysokopoziomowy cel zdefiniowany przez użytkownika.
        router_llm_config: Konfiguracja LLM dla agenta-rutera.

    Returns:
        Obiekt Pydantic `WorkflowPlan` jeśli planowanie się powiodło, w przeciwnym razie None.
    """
    print("\n" + "="*50)
    print("### ROZPOCZYNANIE FAZY PROJEKTOWEJ ###")
    print("="*50)
    
    # --- Krok 1: Dynamiczny Wybór Zespołu przez Rutera ---
    # Delegujemy zadanie wyboru zespołu do modułu Rutera.
    # Zakładamy, że `select_team` jest niezawodny i zwróci poprawny zespół.
    team = select_team(mission, DISCUSSION_AGENT_LIBRARY, router_llm_config)
    planners = team["planners"]
    critic = team["critic"]

    # --- Krok 2: Przygotowanie Środowiska Dyskusji (AutoGen GroupChat) ---
    # Konfigurujemy agenta proxy oraz zasady "ruchu" w rozmowie.
    user_proxy = autogen.UserProxyAgent(
        name="Menedzer_Projektu",
        human_input_mode="NEVER",
        # === POPRAWKA 1: Nasłuchujemy na DOKŁADNĄ, wielkimi literami pisaną frazę ===
        is_termination_msg=lambda x: "PLAN_ZATWIERDZONY" in x.get("content", ""),
        code_execution_config=False
    )
    
    def custom_speaker_selection(last_speaker, groupchat):
        """Wymusza cykl debaty i zatrzymuje go po zatwierdzeniu planu."""
        messages = groupchat.messages
        planner_names = [p.name for p in planners]
        # Jeśli ostatnia wiadomość od Krytyka zawiera zatwierdzenie, zakończ dyskusję
        if last_speaker.name == critic.name and "PLAN_ZATWIERDZONY" in messages[-1].get("content", ""):
            return None # Zwrócenie None elegancko kończy rozmowę

        if last_speaker.name == "Menedzer_Projektu":
            return planners[0]
        
        # Po recenzji krytyka, pętla wraca do pierwszego planisty.
        if last_speaker.name == critic.name:
            return planners[0]

        # Jeśli ostatnio mówił planista...
        if last_speaker.name in planner_names:
            # Znajdź ostatnią turę krytyka.
            last_critic_turn_index = -1
            for i in range(len(messages) - 1, -1, -1):
                if messages[i].get("name") == critic.name:
                    last_critic_turn_index = i
                    break
            
            # Zbierz, którzy planiści już mówili od ostatniej tury krytyka.
            speakers_since_critic = {msg.get("name") for msg in messages[last_critic_turn_index + 1:]}
            
            # Znajdź tych, którzy jeszcze nie mówili w tej rundzie.
            unspoken_planners = [p for p in planners if p.name not in speakers_since_critic]
            
            if unspoken_planners:
                # Jeśli są planiści, którzy jeszcze nie mówili, oddaj głos pierwszemu z nich.
                return unspoken_planners[0]
            else:
                # Jeśli wszyscy planiści już się wypowiedzieli, czas na krytyka.
                return critic
        
        # Domyślny fallback
        return planners[0]

    all_agents = [user_proxy] + planners + [critic]
    groupchat = autogen.GroupChat(
        agents=all_agents, 
        messages=[], 
        max_round=15, 
        speaker_selection_method=custom_speaker_selection
    )
    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=main_agent_configuration)
    
    descriptions = [f"- {name}: {details['description']}" for name, details in NODE_LIBRARY.items()]
    node_descriptions = "Dostępne narzędzia:\n" + "\n".join(descriptions)

    task_config = PromptEngine.for_architect(mission, node_descriptions)
    task_message = PromptEngine.build(task_config)
    
    planner_instance = planners[0]
    prompt_factory_method = PROMPT_FACTORY_MAP.get(type(planner_instance))

    # Zabezpieczenie: jeśli dla agenta (np. GPT) nie ma mapy, użyj ogólnego architekta
    if not prompt_factory_method:
        prompt_factory_method = PromptEngine.for_architect

    task_config = prompt_factory_method(mission, node_descriptions)
    task_message = PromptEngine.build(task_config)

    print("\n--- URUCHAMIANIE BURZY MÓZGÓW ---")
    
    user_proxy.initiate_chat(manager, message=task_message)
    
    # --- Krok 4: Zapis Logów i Ekstrakcja Finalnego Planu ---
    
    # Zapisujemy pełną konwersację do analizy
    os.makedirs(LOGS_DIR, exist_ok=True)

    # NOWA, BARDZIEJ CZYTELNA METODA ZAPISU LOGU Z MYŚLAMI AGENTÓW:
    with open(LOG_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("### ZAPIS PRZEBIEGU DYSKUSJI AGENTÓW (Z PROCESEM MYŚLOWYM) ###\n")
        f.write("="*60 + "\n\n")

        for i, msg in enumerate(groupchat.messages):
            speaker = msg.get("name", "Nieznany")
            content = msg.get("content", "").strip()

            f.write(f"---[ TURA {i+1}: Mówca: {speaker} ]---\n\n")

            try:
                # Spróbuj wyodrębnić JSON z wiadomości
                json_str = content[content.find('{'):content.rfind('}')+1]
                if not json_str: raise ValueError("Brak JSON w wiadomości")
                
                data = json.loads(json_str)
                
                # Jeśli jest proces myślowy, wyodrębnij go i sformatuj
                if "thought_process" in data:
                    thoughts = data.pop("thought_process") # Wyjmij myśli z danych
                    f.write("--- Myśli Agenta ---\n")
                    f.write(thoughts)
                    f.write("\n\n--- Oficjalna Odpowiedź ---\n")
                
                # Zapisz resztę JSON-a (czyli sam plan lub werdykt)
                f.write(json.dumps(data, indent=2, ensure_ascii=False))
                
                # Sprawdź, czy poza JSON-em jest tekst (np. PLAN_ZATWIERDZONY)
                extra_text = content[content.rfind('}')+1:].strip()
                if extra_text:
                    f.write("\n\n" + extra_text)

            except (json.JSONDecodeError, ValueError, IndexError):
                # Jeśli to nie jest JSON, zapisz jako zwykły komunikat
                f.write("--- Komunikat ---\n")
                f.write(content if content else "[Pusta wiadomość]")

            f.write("\n\n" + "="*60 + "\n\n")

    print(f"\nINFO: Pełen, sformatowany log z procesem myślowym zapisano w {LOG_FILE_PATH}")
    final_plan_message = None
    # Przeglądamy wiadomości od końca, aby znaleźć ostatnią wiadomość od krytyka z zatwierdzeniem
    for msg in reversed(groupchat.messages):
        content = msg.get("content", "")
        if "PLAN_ZATWIERDZONY" in content and msg.get("name") == critic.name:
            final_plan_message = content
            break

    if final_plan_message:
        try:
            # Niezawodne wyodrębnianie JSON-a, nawet jeśli LLM dodał dodatkowy tekst
            json_str = final_plan_message[final_plan_message.find('{'):final_plan_message.rfind('}')+1]
            plan_dict = json.loads(json_str)

            validated_plan = WorkflowPlan.model_validate(plan_dict)
            print("\n--- SUKCES: Plan został pomyślnie wygenerowany i zwalidowany. ---")
            return validated_plan
            
        except (json.JSONDecodeError, IndexError) as e:
            print(f"\n--- BŁĄD KRYTYCZNY: Nie udało się wyodrębnić planu z finalnej wiadomości. Błąd: {e} ---")
            return None
            
    print("\n--- ZAKOŃCZONO: Dyskusja zakończyła się bez zatwierdzonego planu. ---")
    return None






def build_and_run_graph(plan: WorkflowPlan, state_schema: Type, initial_state: Dict, stream_config: Dict = None):
    """
    Buduje graf, uruchamia go i GWARANTUJE zapis pełnego logu z wykonania do pliku,
    nawet w przypadku wystąpienia błędu krytycznego.
    """
    if stream_config is None:
        stream_config = {}

    log_file_path = os.path.join("reports", "graph_execution.log")
    os.makedirs("reports", exist_ok=True)
    
    # Używamy bufora w pamięci do przechwytywania logów w czasie rzeczywistym
    log_buffer = io.StringIO()
    original_stdout = sys.stdout
    original_stderr = sys.stderr

    # Zaczynamy główny blok try...finally, aby zagwarantować wykonanie zapisu
    try:
        # Przekierowujemy standardowe wyjście (print) i błędy do naszego bufora
        sys.stdout = log_buffer
        sys.stderr = log_buffer

        print("--- ROZPOCZĘCIE BUDOWY I WYKONANIA GRAFU ---")
        
        workflow = StateGraph(state_schema)

        for node_def in plan.nodes:
            node_function = NODE_LIBRARY[node_def.implementation]['function']
            workflow.add_node(node_def.name, node_function)

        for edge_def in plan.edges:
            source = edge_def.source
            if edge_def.condition:
                condition_function = NODE_LIBRARY[edge_def.condition]['function']
                routes = {k: (END if v == "__end__" else v) for k, v in edge_def.routes.items()}
                workflow.add_conditional_edges(source, condition_function, routes)
            elif edge_def.target:
                target = END if edge_def.target == "__end__" else edge_def.target
                workflow.add_edge(source, target)
        
        workflow.set_entry_point(plan.entry_point)
        app = workflow.compile()

        print("\n--- STRUKTURA ZBUDOWANEGO GRAFU ---")
        app.get_graph().print_ascii()
        
        print("\n--- URUCHOMIENIE GRAFU ---")
        for event in app.stream(initial_state, stream_config):
            print(json.dumps(event, indent=2, ensure_ascii=False))

    except Exception as e:
        # Jeśli wystąpi JAKIKOLWIEK błąd, również go zapisujemy do bufora
        print(f"\n--- BŁĄD KRYTYCZNY: PRZERWANO WYKONANIE ---")
        import traceback
        traceback.print_exc()

    finally:
        # Ten blok wykona się ZAWSZE, niezależnie od tego, czy był błąd, czy nie.
        
        # 1. Przywracamy oryginalne wyjścia, aby konsola znowu działała normalnie
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        
        # 2. Pobieramy całą zawartość z bufora
        log_content = log_buffer.getvalue()
        
        # 3. Zapisujemy zawartość bufora do pliku
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        # 4. Wyświetlamy ostateczny log w konsoli
        print("\n" + "="*60)
        print(f"INFO: Pełen log z wykonania grafu został zapisany w: {log_file_path}")
        print("--- KONSOLA: Poniżej znajduje się ostateczna zawartość logu ---")
        print("="*60)
        print(log_content)




--- FILE: dynamic_graph/nodes.py ---

import pandas as pd
import numpy as np



# Tworzymy pusty rejestr i dekorator
NODE_LIBRARY = {}
def node(name: str, description: str):
    def decorator(func):
        NODE_LIBRARY[name] = {
            "function": func,
            "description": description
        }
        return func
    return decorator


# --- Węzły-Zaślepki ---
@node(name="load_data", description="Wczytuje dane z pliku CSV.")
def load_data_node(state: dict) -> dict:
    print("-> EXECUTING: load_data_node (ZAŚLEPKA)")
    return state

@node(name="discover_causality", description="Uruchamia agenta do odkrywania przyczynowości. UWAGA: Ta operacja jest złożona i może zakończyć się błędem.")
def discover_causality_node(state: dict) -> dict:
    print("-> EXECUTING: discover_causality_node (ZAŚLEPKA)")
    return state

@node(name="validate_model", description="Przeprowadza testy walidacyjne na modelu.")
def validate_model_node(state: dict) -> dict:
    print("-> EXECUTING: validate_model_node (ZAŚLEPKA)")
    return state

@node(name="universal_debugger", description="Narzędzie do diagnozowania i naprawiania błędów, które wystąpiły w innych węzłach.")
def universal_debugger_node(state: dict) -> dict:
    print("-> EXECUTING: universal_debugger_node (ZAŚLEPKA)")
    return state

@node(name="human_escalation", description="Narzędzie do eskalacji problemu do operatora ludzkiego, gdy automatyczna naprawa zawiedzie.")
def human_escalation_node(state: dict) -> dict:
    print("-> EXECUTING: human_escalation_node (ZAŚLEPKA)")
    return state

@node(name="check_for_error", description="Węzeł warunkowy. Sprawdza, czy poprzedni krok zakończył się błędem. Zwraca 'error' lub 'success'.")
def check_for_error(state: dict) -> str:
    print("... CONDITION: check_for_error (ZAŚLEPKA) -> 'success'")
    return "success"
    
    
@node(name="generate_explainability_report", description="Generuje finalny raport wyjaśnialności, podsumowujący cały proces decyzyjny.")
def generate_report_node(state: dict) -> dict:
    """
    Zbiera artefakty ze stanu i tworzy spójny raport w formacie Markdown.
    """
    return state



--- FILE: dynamic_graph/outputsModels.py ---

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Type


class AgentSelection(BaseModel):
    """Struktura odpowiedzi dla Agenta-Rutera."""
    thought_process: str = Field(description="Krótki proces myślowy, dlaczego wybrano tych konkretnych agentów do zadania.")
    planners: List[str] = Field(description="Lista nazw agentów-planistów wybranych do zadania.")
    critic: str = Field(description="Nazwa agenta-krytyka wybranego do zadania.")

class NodeDefinition(BaseModel):
    name: str = Field(description="Unikalna nazwa węzła w grafie.")
    implementation: str = Field(description="Nazwa funkcji z biblioteki narzędzi, która realizuje ten węzeł.")

class EdgeDefinition(BaseModel):
    source: str = Field(alias="from", description="Nazwa węzła źródłowego.")
    target: Optional[str] = Field(alias="to", default=None, description="Nazwa węzła docelowego dla prostych krawędzi.")
    condition: Optional[str] = Field(default=None, description="Nazwa funkcji warunkowej z biblioteki narzędzi.")
    routes: Optional[Dict[str, str]] = Field(default=None, description="Mapa wyników warunku na nazwy węzłów docelowych.")

class WorkflowPlan(BaseModel):
    """Struktura odpowiedzi dla Agenta-Architekta. To jest finalny plan grafu."""
    thought_process: str = Field(description="Proces myślowy krok-po-kroku, który doprowadził do zaprojektowania tego konkretnego grafu.")
    entry_point: str = Field(description="Nazwa węzła, od którego zaczyna się przepływ pracy.")
    nodes: List[NodeDefinition]
    edges: List[EdgeDefinition]



--- FILE: dynamic_graph/router.py ---

import autogen
from typing import Optional, Dict, Any,Type
from pydantic import BaseModel, ValidationError
import json
from prompts import *

def get_structured_response(
    prompt: str, 
    response_model: Type[BaseModel], 
    llm_config: Dict, 
    max_retries: int = 3
) -> Optional[BaseModel]:
    """
    Wywołuje dowolnego agenta LLM i próbuje sparsować jego odpowiedź do modelu Pydantic.
    W przypadku błędu, prosi agenta o autokorektę.
    """
    # Używamy prostego agenta jako "wykonawcy" dla dowolnej konfiguracji LLM
    executor_agent = autogen.ConversableAgent("Executor", llm_config=llm_config)
    
    current_prompt = prompt
    for attempt in range(max_retries):
        print(f"INFO [StrucRes]: Próba {attempt + 1}/{max_retries}...")
        
        reply_dict  = executor_agent.generate_reply(messages=[{"role": "user", "content": current_prompt}])
        reply_content = reply_dict.get("content", "")
        try:
            # Próbujemy znaleźć i sparsować JSON z odpowiedzi
            json_str =reply_content[reply_content.find('{'):reply_content.rfind('}')+1]
            if not json_str:
                raise json.JSONDecodeError("Nie znaleziono obiektu JSON w odpowiedzi.", reply_content, 0)
            json_obj = json.loads(json_str)
            
            # Walidujemy, czy pasuje do naszego modelu Pydantic
            validated_obj = response_model.model_validate(json_obj)
            print("INFO [StrucRes]: Odpowiedź poprawna i zwalidowana.")
            return validated_obj
            
        except (json.JSONDecodeError, ValidationError) as e:
            print(f"OSTRZEŻENIE [StrucRes]: Odpowiedź LLM nie jest poprawnym obiektem. Błąd: {e}")
            # Tworzymy nowy prompt z prośbą o autokorektę
            current_prompt = f"{prompt}\n\nTwoja poprzednia odpowiedź była niepoprawna: '{reply}'. Proszę, popraw ją i zwróć TYLKO I WYŁĄCZNIE poprawny obiekt JSON."
    
    print("BŁĄD [StrucRes]: Nie udało się uzyskać poprawnej odpowiedzi po maksymalnej liczbie prób.")
    return None



def select_team(user_query: str, agent_library: Dict, router_llm_config: Dict) -> Dict[str, Any]:
    """Dynamicznie wybiera zespół, używając uniwersalnej funkcji do odpowiedzi strukturalnych."""
    print("--- RUTER AGENTÓW: Uruchamiam uniwersalny wybór zespołu... ---")
    
    agent_descriptions_list = []
    for name, agent in agent_library.items():
        try:
            # Dzielimy po nowym, ustrukturyzowanym nagłówku
            specialization_text = agent.system_message.split("## SPECIALIZATION:")[1]
            # Bierzemy tylko pierwszą linijkę opisu, aby był zwięzły
            description = specialization_text.strip().split('\n')[0]
            agent_descriptions_list.append(f"- {name}: {description}")
        except IndexError:
            # Zabezpieczenie, jeśli jakiś prompt nie ma tej sekcji
            agent_descriptions_list.append(f"- {name}: Brak opisu specjalizacji.")

    agent_descriptions = "\n".join(agent_descriptions_list)
    # Tworzymy konfigurację i budujemy prompt tak jak poprzednio
    router_config = PromptEngine.for_router(user_query, agent_descriptions)
    router_prompt = PromptEngine.build(router_config)
    
    # NOWOŚĆ: Wywołujemy naszą uniwersalną funkcję
    selection_obj = get_structured_response(
        prompt=router_prompt,
        response_model=AgentSelection,
        llm_config=router_llm_config
    )
    
    if selection_obj:
        selection = selection_obj.model_dump()
        print(f"--- RUTER AGENTÓW: Wybrany zespół -> {selection} ---")
        return {
            "planners": [agent_library[name] for name in selection["planners"]],
            "critic": agent_library[selection["critic"]]
        }
    else:
        # Tryb awaryjny, jeśli LLM nie zwrócił poprawnej odpowiedzi
        print("--- RUTER AGENTÓW: Działam w trybie awaryjnym. Wybieram domyślny zespół. ---")
        return {
            "planners": [agent_library["Analityk_Danych"]],
            "critic": agent_library["Krytyk_Jakosci"]
        }



--- FILE: dynamic_graph/state.py ---

from typing import TypedDict,List, Dict, Any, Optional, Type
import pandas as pd
# Definiujemy strukturę danych, które będą krążyć w naszym grafie
class WorkflowState(TypedDict):
    mission: str
    input_path: str
    dataframe: pd.DataFrame
    causal_graph: Any
    validation_report: Dict
    final_summary: str
    error: str
    # NOWE POLA DLA AGENTA-SPRAWOZDAWCY
    router_decision: Dict  # Przechowa 'thought_process' i wybór Rutera
    architect_plan: Dict   # Przechowa pełny, zwalidowany plan (WorkflowPlan)
    planning_log: List[Dict] # Przechowa pełną rozmowę z "burzy mózgów"
    lime_shap_report_html: str  # Przechowa wizualizację LIME/SHAP jako HTML
    counterfactual_analysis: Dict # Przechowa wynik analizy "co by było, gdyby?"


--- FILE: agents/__init__.py ---




--- FILE: agents/agents.py ---

import autogen
from autogen import Agent, ConversableAgent

# --- Agenci-Dyskutanci (dla Fazy Projektowania - AutoGen) ---

class CausalExpertAgent(ConversableAgent):
    """Agent specjalizujący się w wysokopoziomowych planach przyczynowych."""
    def __init__(self, llm_config, prompt):
        super().__init__(name="Ekspert_Przyczynowosci", llm_config=llm_config, system_message=prompt)

class DataScientistAgent(ConversableAgent):
    """Agent tworzący szczegółowe, techniczne plany przygotowania danych."""
    def __init__(self, llm_config, prompt):
        super().__init__(name="Analityk_Danych", llm_config=llm_config, system_message=prompt)

class ArchitectGPTAgent(ConversableAgent):
    """Agent-Architekt oparty na modelu GPT, specjalizujący się w projektowaniu odpornych grafów."""
    def __init__(self, llm_config, prompt):
        super().__init__(name="Architekt_GPT", llm_config=llm_config, system_message=prompt)

class CriticAgent(ConversableAgent):
    """Agent oceniający plan i dbający o jego jakość."""
    def __init__(self, llm_config, prompt):
        super().__init__(name="Krytyk_Jakosci", llm_config=llm_config, system_message=prompt)


# --- Klasa Bazowa dla "Robotników" (zostaje na przyszłość) ---
class WorkerAgent:
    """Klasa bazowa dla agentów, które działają jako węzły w grafie LangGraph."""
    def __init__(self, llm_client, persona: str):
        self.llm = llm_client
        self.persona = persona

    def execute(self, state: dict) -> dict:
        raise NotImplementedError("Każdy WorkerAgent musi mieć zaimplementowaną metodę execute.")


--- FILE: agents/agents_library.py ---

from prompts import (
    SYSTEM_PROMPT_ANALYST,
    SPEC_CAUSAL_EXPERT,
    SPEC_DATA_ANALYST,
    SPEC_CRITIC_AEGIS,
    SPEC_SELF_HEALING_CAPABILITY,
    SPEC_ARCHITECT_GPT
)
from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, GPT_MODEL, API_TYPE_OPENAI, OPENAI_API_KEY, API_TYPE_GEMINI,API_TYPE_SONNET, ANTHROPIC_API_KEY,basic_config_agent
from .agents import *


# --- Konfiguracja czatu grupowego ---
main_agent_configuration={"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}
critic_agent_configuration ={"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}


openai_agent_configuration = {
    "config_list": basic_config_agent(
        agent_name=GPT_MODEL,
        api_type=API_TYPE_OPENAI
    ),
    "temperature": 0.0,
    "cache_seed": 42
}

# --- Budujemy Kompletne Persony ---
causal_expert_persona = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_CAUSAL_EXPERT}"
data_scientist_persona = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_DATA_ANALYST}"
critic_persona = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_CRITIC_AEGIS}"
architect_gpt_persona = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_ARCHITECT_GPT}"

# --- Tworzymy Instancje Agentów-Dyskutantów ---
planner_causal = CausalExpertAgent(llm_config=main_agent_configuration, prompt=causal_expert_persona)
planner_data = DataScientistAgent(llm_config=main_agent_configuration, prompt=data_scientist_persona)
architect_gpt = ArchitectGPTAgent(llm_config=openai_agent_configuration, prompt=architect_gpt_persona)
critic = CriticAgent(llm_config=critic_agent_configuration, prompt=critic_persona)

# --- Biblioteka dla Rutera ---
DISCUSSION_AGENT_LIBRARY = {
    "Ekspert_Przyczynowosci": planner_causal,
    "Analityk_Danych": planner_data,
    "Architekt_GPT": architect_gpt,
    "Krytyk_Jakosci": critic,
}


# causal_discovery_worker = CausalDiscoveryAgent()
# model_validation_worker = ModelValidationAgent()

# WORKER_AGENT_LIBRARY = {
#     "discover_causality_worker": causal_discovery_worker,
#     "validate_model_worker": model_validation_worker
# }



--- FILE: agents/llm_clients.py ---

from langchain_google_vertexai import ChatVertexAI
from langchain_anthropic import ChatAnthropic
from pydantic import BaseModel
from typing import Type
import outlines
from config import (
    MAIN_AGENT,
    CODE_MODEL,
    CRITIC_MODEL,
    PROJECT_ID,
    LOCATION
)

# Sekcja 1: Fabryki dla Bazowych Klientów LLM
# Te funkcje tworzą standardowe, "surowe" klienty LangChain.
# Wzorzec "singleton" (zmienna globalna _...) zapewnia, że każdy
# klient jest tworzony tylko raz, co oszczędza zasoby.
# ======================================================================


_main_llm_client = None
def get_main_llm():
    global _main_llm_client
    if _main_llm_client is None:
        _main_llm_client = ChatVertexAI(
            model_name=MAIN_AGENT,
            temperature=0.0,
            project=PROJECT_ID,
            location=LOCATION
        )
    return _main_llm_client


_code_llm_client = None
def get_code_llm():
    global _code_llm_client
    if _code_llm_client is None:
        _code_llm_client = ChatAnthropic(
            model_name=CODE_MODEL,
            temperature=0.0,
            max_tokens=4096
        )
    return _code_llm_client

_critic_llm_client = None
def get_critic_llm():
    global _critic_llm_client
    if _critic_llm_client is None:
        _critic_llm_client = ChatAnthropic(
            model_name=CRITIC_MODEL,
            temperature=0.0,
            max_tokens=2048
        )
    return _critic_llm_client


# ======================================================================
# Sekcja 2: "Wzmacniacz" z Biblioteki Outlines
# Ta funkcja jest sercem Twojego nowego, niezawodnego systemu.
# ======================================================================

def get_enforced_llm_generator(base_llm_client, pydantic_schema: Type[BaseModel]):
    """
    Uniwersalna funkcja, która bierze dowolnego klienta LangChain i "opakowuje"
    go w mechanizm wymuszania schematu Pydantic za pomocą biblioteki Outlines.

    Args:
        base_llm_client: Obiekt klienta LangChain (np. z get_main_llm()).
        pydantic_schema: Klasa Pydantic, której format ma być wymuszony.

    Returns:
        Funkcja-generator gotowa do wywołania z promptem.
    """
    # 1. Tworzymy model kompatybilny z Outlines na bazie klienta LangChain
    model = outlines.models.langchain(base_llm_client)
    
    # 2. Tworzymy generator, który wymusza konkretny schemat Pydantic
    generator = outlines.generate.json(model, pydantic_schema)
    
    # 3. Zwracamy gotową do użycia funkcję-generator
    return generator



