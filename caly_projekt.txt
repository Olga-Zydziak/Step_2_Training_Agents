--- FILE: __init__.py ---




--- FILE: config.py ---

import os
import logging
from enum import Enum
from google.cloud import secretmanager
import langchain
from langchain.cache import SQLiteCache





def get_secret(project_id: str, secret_id: str, version_id: str = "latest") -> str:
    """Pobiera wartość sekretu z Google Secret Manager."""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})
   
    return response.payload.data.decode("UTF-8")


class ApiType(Enum):
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    def __str__(self):
        return self.value


LOCATION="us-central1"
PROJECT_ID="dark-data-discovery"

#---------AGENTS--------:
MAIN_AGENT="gemini-2.5-pro"
API_TYPE_GEMINI=str(ApiType.GOOGLE)

CRITIC_MODEL="claude-3-7-sonnet-20250219"
CODE_MODEL="claude-sonnet-4-20250514"
QUICK_SMART_MODEL="gemini-2.5-flash"
API_TYPE_SONNET = str(ApiType.ANTHROPIC)

LANGCHAIN_API_KEY = get_secret(PROJECT_ID,"LANGCHAIN_API_KEY")
ANTHROPIC_API_KEY=get_secret(PROJECT_ID,"ANTHROPIC_API_KEY")

MEMORY_ENGINE_DISPLAY_NAME="memory-gamma-way"

INPUT_FILE_PATH = "gs://super_model/data/structural_data/synthetic_fraud_dataset.csv"

MAX_CORRECTION_ATTEMPTS=5



os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "Projekt Multi-Agent-System v9.0-Integrated"
os.environ["ANTHROPIC_API_KEY"] =ANTHROPIC_API_KEY


#---cache-------
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")




#FUNKCJA KONFIGURACYJNA AGENTOW AUTOGEN
def basic_config_agent(agent_name:str, api_type:str, location:str=None, project_id:str=None, api_key:str=None):
    try:
        configuration = {"model": agent_name, "api_type": api_type}
        if api_key: configuration["api_key"] = api_key
        if project_id: configuration["project_id"] = project_id
        if location: configuration["location"] = location
        logging.info(f"Model configuration: {configuration}")
        return [configuration]

    except Exception as e:
        logging.error(f"Failed to initialize Vertex AI or configure LLM: {e}")
        print(f"Error: Failed to initialize Vertex AI or configure LLM. Please check your project ID, region, and permissions. Details: {e}")
        exit()



--- FILE: main.ipynb ---

from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET,QUICK_SMART_MODEL,ANTHROPIC_API_KEY,basic_config_agent
from dynamic_graph.outputsModels import *
from dynamic_graph.main_graph_functions import run_collaborative_planning
from prompts import PromptEngine
# --- Koniec komórki ---
# --- Konfiguracja czatu grupowego ---
main_agent_configuration={"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}
critic_agent_configuration ={"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}
router_agent_configuration = {"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(agent_name=QUICK_SMART_MODEL, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}
# --- Koniec komórki ---
if __name__ == '__main__':
    # Konfiguracja dla lekkiego i szybkiego modelu Rutera
    
    
    # Wysokopoziomowa misja od użytkownika
    mission = """
    Zbuduj prosty, sekwencyjny przepływ pracy do weryfikacji modelu przyczynowego.
    Załóż, że każdy krok (węzeł) kończy się natychmiast (działa synchronicznie).
    Plan powinien składać się z kroków: load_data, discover_causality, a na końcu validate_model.
    """
    
    # Uruchom całą fazę planowania jedną komendą
    final_plan = run_collaborative_planning(mission, router_agent_configuration)
    
    if final_plan:
        print("\n" + "="*50)
        print("### OTRZYMANY PLAN GOTOWY DO WYKONANIA ###")
        print(final_plan.model_dump_json(indent=2, by_alias=True))
        
        # W kolejnym kroku, ten obiekt `final_plan` zostałby przekazany
        # do "Fabryki Grafów", aby zbudować i uruchomić przepływ.
        # build_and_run_graph(final_plan, ...)
    else:
        print("\nNie udało się wypracować planu. Sprawdź logi w `reports/`.")

    
    
# --- Koniec komórki ---

# --- Koniec komórki ---



--- FILE: prompts.py ---

import json
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Type
from dynamic_graph.outputsModels import *
# =================================================================================
# Sekcja 1: DYREKTYWY SYSTEMOWE (PERSONY NADRZĘDNE)
# Skopiowane z Twojego pliku prompts_beta (1).py
# =================================================================================

SYSTEM_PROMPT_ANALYST = """
# CORE DIRECTIVE: STRATEGIC AI ANALYST "ORACLE"
Jesteś "Oracle", elitarnym analitykiem AI specjalizującym się w strategii, analizie i krytycznym myśleniu. Twoim celem jest przetwarzanie złożonych informacji, podejmowanie trafnych decyzji i komunikowanie wniosków z absolutną precyzją.

## CORE PRINCIPLES (NON-NEGOTIABLE)
1.  **Structured Thinking:** Zanim sformułujesz finalną odpowiedź, rozpisz swój proces myślowy krok po kroku w polu `thought_process`.
2.  **Evidence-Based Reasoning:** Twoje decyzje i oceny muszą być oparte wyłącznie na dostarczonych danych (`<CONTEXT>`).
3.  **Adherence to Format:** Ściśle przestrzegaj wymaganego formatu wyjściowego. Od tego zależy stabilność całego systemu.
"""


SPEC_CRITIC_AEGIS = """
## SPECIALIZATION: AEGIS, THE UNYIELDING GUARDIAN OF QUALITY
Twoją wyspecjalizowaną funkcją jest "Aegis" – tarcza chroniąca system przed błędami. Twoim celem jest aktywne wzmacnianie i hartowanie każdego planu poprzez precyzyjną, konstruktywną analizę.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)

1.  **Deep Analysis vs. Superficial Validation:** Nigdy nie akceptuj planu tylko dlatego, że "spełnia minimalne wymagania". Zawsze szukaj ukrytych wad i potencjalnych ryzyk.
2.  **Absolute Adherence to Format:** Twoja odpowiedź ZAWSZE musi być poprawnym obiektem JSON. Musisz także bezwzględnie przestrzegać ZŁOTEJ ZASADY ZAKOŃCZENIA PRACY.

### STEP-BY-STEP THOUGHT PROCESS
1.  **Formal Verification:** Sprawdzenie poprawności składni JSON i zgodności ze schematem.
2.  **Logical & Mission-Based Analysis:** Weryfikacja zgodności z celem z `<MISSION>`.
3.  **Architectural Assessment (Quality Heuristics):** Ocena planu pod kątem prostoty, odporności na błędy, efektywności i czytelności.
4.  **Verdict Formulation:** Podjęcie ostatecznej decyzji i wybór jednego z dwóch trybów działania.

### TWO MODES OF OPERATION
#### Mode 1: FLAW DETECTED (CORRECTION NEEDED)
Jeśli plan ma wadę, opisz ją i zasugeruj rozwiązanie.

#### Mode 2: FINAL PLAN (APPROVAL)
Jeśli plan jest bezbłędny, zatwierdź go i zakończ pracę.

### THE GOLDEN RULE OF TERMINATION
**JEŚLI I TYLKO JEŚLI ZATWIERDZASZ PLAN (Tryb 2), MUSISZ dodać do swojej odpowiedzi, poza blokiem JSON, kluczową frazę ostatecznego zatwierdzenia: `PLAN_ZATWIERDZONY`**
"""

SPEC_ROUTER = """
## SPECIALIZATION: TEAM COMPOSITION STRATEGIST
Twoją specjalizacją jest strategiczna analiza wymagań misji w celu skomponowania optymalnego zespołu agentów-ekspertów. Działasz jak dyrektor castingu, dobierając odpowiednie talenty do zadania. Twoja decyzja musi być oparta wyłącznie na opisie misji i charakterystyce dostępnych agentów.
"""


SPEC_ARCHITECT = """
## SPECIALIZATION: WORKFLOW ARCHITECT
Twoją specjalizacją jest tłumaczenie wysokopoziomowych misji na precyzyjne, maszynowo-czytelne plany przepływu pracy (workflow) w formacie JSON. Jesteś głównym inżynierem systemu.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)

1.  **Modularity over Monoliths:** Projektuj przepływy jako serię małych, wyspecjalizowanych kroków (węzłów), które można łatwo zrozumieć i testować.
2.  **Plan for Failure:** Zawsze zastanów się, co może pójść nie tak. Nawet jeśli misja wymaga prostego przepływu, w procesie myślowym odnotuj potencjalne punkty awarii.
3.  **Clarity and Readability:** Nazwy węzłów i struktura grafu muszą być intuicyjne i łatwe do zrozumienia dla innego inżyniera.
"""


SPEC_CAUSAL_EXPERT = """
## SPECIALIZATION: CAUSAL MODELING ARCHITECT
Twoim zadaniem jest projektowanie wysokopoziomowych planów przepływu pracy (`WorkflowPlan`) dla modeli przyczynowo-skutkowych. Koncentrujesz się na logicznej sekwencji kroków, a nie na szczegółach implementacji.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)
1.  **Logic First:** Skup się na poprawnym uporządkowaniu głównych etapów (np. `load_data` -> `discover_causality` -> `validate_model`).
2.  **Simplicity:** Twój plan powinien być tak prosty i przejrzysty, jak to tylko możliwe. Unikaj zbędnych, skomplikowanych ścieżek, jeśli misja tego nie wymaga.
3.  **Clarity:** Każdy węzeł i krawędź w Twoim planie musi mieć jasny i zrozumiały cel.
"""

SPEC_DATA_ANALYST = """
## SPECIALIZATION: DATA PREPARATION PLANNER
Twoim zadaniem jest tworzenie szczegółowych, technicznych planów przepływu pracy (`WorkflowPlan`) dotyczących czyszczenia, transformacji i walidacji danych.

### ADDITIONAL CORE PRINCIPLES (ROLE-SPECIFIC)
1.  **Data-Driven Decisions:** Każdy krok w Twoim planie musi wynikać bezpośrednio z analizy schematu danych (np. nazw i typów kolumn).
2.  **Granularity:** Dziel złożone operacje na mniejsze, atomowe kroki. Zamiast jednego węzła "przetwórz dane", stwórz osobne węzły dla "imputacji braków", "korekty typów" i "inżynierii cech".
3.  **Justification:** W polu `thought_process` krótko uzasadnij, dlaczego dany krok jest potrzebny (np. "Kolumna 'X' ma 30% braków, dlatego dodaję węzeł imputacji medianą").
"""




class PromptConfig(BaseModel):
    """Generyczna struktura do konfigurowania dowolnego promptu."""
    system_prompt: str
    task_description: str
    context: Dict[str, Any] = Field(default_factory=dict)
    output_schema: Type[BaseModel]

class PromptEngine:
    """Centralny silnik do generowania promptów."""

    @staticmethod
    def build(config: PromptConfig) -> str:
        """Składa finalny, kompletny prompt jako string."""
        prompt = [config.system_prompt]
        prompt.append(f"## TASK\n{config.task_description}")
        if config.context:
            context_str = "\n".join(f"<{key.upper()}>\n{value}\n</{key.upper()}>" for key, value in config.context.items() if value)
            prompt.append(f"## CONTEXT\n{context_str}")
        
        schema_json = json.dumps(config.output_schema.model_json_schema(), indent=2)
        output_format_str = f"Twoja odpowiedź MUSI być poprawnym obiektem JSON zgodnym z poniższym schematem. Nie dodawaj żadnych innych treści poza obiektem JSON.\n```json\n{schema_json}\n```"
        prompt.append(f"## OUTPUT FORMAT\n{output_format_str}")
        return "\n\n".join(prompt)

    # --- Metody do tworzenia gotowych konfiguracji dla Fabryki Agentów ---

    
    
    @staticmethod
    def for_critic() -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Krytyka 'Aegis'."""

        # 1. Budujemy kompletny system_prompt dla Krytyka
        system_prompt_for_critic = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_CRITIC_AEGIS}"

        # 2. "Pakujemy" wszystko do obiektu PromptConfig
        return PromptConfig(
            system_prompt=system_prompt_for_critic,
            task_description="",  # Zadanie jest wbudowane w system_prompt
            context={},            # Kontekst to historia czatu, nic więcej nie potrzeba
            output_schema=WorkflowPlan  # Schemat jest potrzebny do walidacji
        )

    
    @staticmethod
    def for_router(mission: str, agent_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Rutera."""

        # Budujemy kompletny system_prompt, łącząc fundament ze specjalizacją
        system_prompt_for_router = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_ROUTER}"

        # Opis zadania, które agent ma wykonać
        task_description = "Na podstawie poniższej misji i listy dostępnych ekspertów, dobierz optymalny zespół składający się z 1-2 planistów oraz 1 krytyka."

        # Kontekst jest dynamiczny i niezbędny do wykonania zadania
        context = {
            "mission": mission,
            "available_experts": agent_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt_for_router,
            task_description=task_description,
            context=context,
            output_schema=AgentSelection  # Agent ma zwrócić obiekt zgodny z tym schematem
        )
    
    
    
    @staticmethod
    def for_architect(mission: str, node_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Architekta."""

        # Budujemy kompletny system_prompt
        system_prompt_for_architect = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_ARCHITECT}"

        # Opis zadania do wykonania
        task_description = "Zaprojektuj kompletny i odporny na błędy przepływ pracy (workflow) w formacie JSON, który realizuje zadaną misję, korzystając z dostępnych narzędzi."

        # Kontekst jest niezbędny do zaprojektowania planu
        context = {
            "mission": mission,
            "available_tools": node_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt_for_architect,
            task_description=task_description,
            context=context,
            output_schema=WorkflowPlan # Agent musi zwrócić obiekt `WorkflowPlan`
        )
    @staticmethod
    def for_causal_expert(mission: str, node_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Eksperta od Modeli Przyczynowych."""

        # Łączymy fundament "Oracle" ze specjalizacją
        system_prompt = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_CAUSAL_EXPERT}"

        # Opis zadania, które agent ma wykonać
        task_description = "Zaprojektuj wysokopoziomowy, logiczny plan przepływu pracy (`WorkflowPlan`) dla modelu przyczynowo-skutkowego, opierając się na misji i dostępnych narzędziach."

        # Kontekst, którego potrzebuje do wykonania zadania
        context = {
            "mission": mission,
            "available_tools": node_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt,
            task_description=task_description,
            context=context,
            output_schema=WorkflowPlan  # Oczekiwany format wyjściowy to plan grafu
        )
    
    
    @staticmethod
    def for_data_analyst(mission: str, node_library_descriptions: str) -> PromptConfig:
        """Tworzy konfigurację promptu dla Agenta-Analityka Danych."""

        # Łączymy fundament "Oracle" ze specjalizacją
        system_prompt = f"{SYSTEM_PROMPT_ANALYST}\n\n{SPEC_DATA_ANALYST}"

        # Opis zadania, które agent ma wykonać
        task_description = "Zaprojektuj szczegółowy, techniczny plan przepływu pracy (`WorkflowPlan`) dotyczący czyszczenia i walidacji danych, opierając się na misji i dostępnych narzędziach."

        # Kontekst, którego potrzebuje do wykonania zadania
        context = {
            "mission": mission,
            "available_tools": node_library_descriptions
        }

        # Zwracamy gotowy do użycia "pakiet" konfiguracyjny
        return PromptConfig(
            system_prompt=system_prompt,
            task_description=task_description,
            context=context,
            output_schema=WorkflowPlan  # Ten agent również ma za zadanie stworzyć plan grafu
        )


--- FILE: dynamic_graph/main_graph_functions.py ---

import json
import os
import autogen
from typing import List, Dict, Any, Optional, Type
from .router import get_structured_response,select_team
from agents.agents_library import AGENT_LIBRARY,main_agent_configuration
from .outputsModels import WorkflowPlan
from prompts import *
LOGS_DIR = "reports"
LOG_FILE_PATH = os.path.join(LOGS_DIR, "planning_brainstorm.log")

def run_collaborative_planning(mission: str, router_llm_config: Dict) -> Optional[WorkflowPlan]:
    """
    Orkiestruje całym procesem autonomicznego planowania.

    Jako "reżyser" fazy projektowej, ta funkcja zarządza całym cyklem:
    1. Dynamicznie dobiera zespół agentów-ekspertów za pomocą inteligentnego Rutera.
    2. Konfiguruje ustrukturyzowaną "burzę mózgów" z jasno określonymi zasadami.
    3. Inicjuje dyskusję, przekazując agentom precyzyjnie zbudowany prompt.
    4. Po zakończeniu dyskusji, waliduje i wyodrębnia jej finalny produkt - obiekt WorkflowPlan.

    Args:
        mission: Wysokopoziomowy cel zdefiniowany przez użytkownika.
        router_llm_config: Konfiguracja LLM dla agenta-rutera.

    Returns:
        Obiekt Pydantic `WorkflowPlan` jeśli planowanie się powiodło, w przeciwnym razie None.
    """
    print("\n" + "="*50)
    print("### ROZPOCZYNANIE FAZY PROJEKTOWEJ ###")
    print("="*50)
    
    # --- Krok 1: Dynamiczny Wybór Zespołu przez Rutera ---
    # Delegujemy zadanie wyboru zespołu do modułu Rutera.
    # Zakładamy, że `select_team` jest niezawodny i zwróci poprawny zespół.
    team = select_team(mission, AGENT_LIBRARY, router_llm_config)
    planners = team["planners"]
    critic = team["critic"]

    # --- Krok 2: Przygotowanie Środowiska Dyskusji (AutoGen GroupChat) ---
    # Konfigurujemy agenta proxy oraz zasady "ruchu" w rozmowie.
    user_proxy = autogen.UserProxyAgent(
        name="Menedzer_Projektu",
        human_input_mode="NEVER",
        is_termination_msg=lambda x: "plan_zatwierdzony" in x.get("content", "").lower(),
        code_execution_config=False
    )
    
    def custom_speaker_selection(last_speaker, groupchat):
        """Wymusza cykl debaty: Planista -> Krytyk -> Planista."""
        if last_speaker.name == "Menedzer_Projektu": return planners[0]
        if last_speaker.name in [p.name for p in planners]: return critic
        return planners[0]

    all_agents = [user_proxy] + planners + [critic]
    groupchat = autogen.GroupChat(
        agents=all_agents, 
        messages=[], 
        max_round=12, 
        speaker_selection_method=custom_speaker_selection
    )
    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=main_agent_configuration)
    
   
    node_descriptions = "Dostępne narzędzia: ['load_data', 'discover_causality', 'validate_model', 'check_status']"
    architect_config = PromptEngine.for_architect(mission, node_descriptions)
    architect_prompt = PromptEngine.build(architect_config)
    
    print("\n--- URUCHAMIANIE BURZY MÓZGÓW ---")
    user_proxy.initiate_chat(manager, message=architect_prompt)
    
    # --- Krok 4: Zapis Logów i Ekstrakcja Finalnego Planu ---
    
    # Zapisujemy pełną konwersację do analizy
    os.makedirs(LOGS_DIR, exist_ok=True)
    with open(LOG_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(groupchat.messages, f, indent=2, ensure_ascii=False)
    print(f"\nINFO: Pełen log dyskusji zapisano w {LOG_FILE_PATH}")

    final_plan_message = None
    # Przeglądamy wiadomości od końca, aby znaleźć ostatnią wiadomość od krytyka z zatwierdzeniem
    for msg in reversed(groupchat.messages):
        content = msg.get("content", "")
        if "PLAN_ZATWIERDZONY" in content and msg.get("name") == critic.name:
            final_plan_message = content
            break

    if final_plan_message:
        try:
            # Niezawodne wyodrębnianie JSON-a, nawet jeśli LLM dodał dodatkowy tekst
            json_str = final_plan_message[final_plan_message.find('{'):final_plan_message.rfind('}')+1]
            plan_dict = json.loads(json_str)

            validated_plan = WorkflowPlan.model_validate(plan_dict)
            print("\n--- SUKCES: Plan został pomyślnie wygenerowany i zwalidowany. ---")
            return validated_plan
            
        except (json.JSONDecodeError, IndexError) as e:
            print(f"\n--- BŁĄD KRYTYCZNY: Nie udało się wyodrębnić planu z finalnej wiadomości. Błąd: {e} ---")
            return None
            
    print("\n--- ZAKOŃCZONO: Dyskusja zakończyła się bez zatwierdzonego planu. ---")
    return None






def build_and_run_graph(plan: WorkflowPlan, state_schema: Type, initial_state: Dict):
    """Buduje i uruchamia graf na podstawie planu."""
    # Tutaj wklejamy kod Fabryki Grafów, który już znamy
    workflow = StateGraph(state_schema)
    for node_def in plan.nodes:
        workflow.add_node(node_def.name, NODE_LIBRARY[node_def.implementation])
    for edge_def in plan.edges:
        source = edge_def.source
        if edge_def.condition:
            condition = NODE_LIBRARY[edge_def.condition]
            routes = {k: (END if v == "__end__" else v) for k, v in edge_def.routes.items()}
            workflow.add_conditional_edges(source, condition, routes)
        elif edge_def.target:
            target = END if edge_def.target == "__end__" else edge_def.target
            workflow.add_edge(source, target)
    workflow.set_entry_point(plan.entry_point)
    
    app = workflow.compile()
    print("\n--- STRUKTURA ZBUDOWANEGO GRAFU ---")
    app.get_graph().print_ascii()
    
    print("\n--- URUCHOMIENIE GRAFU ---")
    for event in app.stream(initial_state):
        print(event)



--- FILE: dynamic_graph/nodes.py ---

def load_data_node(state: dict) -> dict: print("-> EXECUTING: load_data_node"); return state
def discover_causality_node(state: dict) -> dict: print("-> EXECUTING: discover_causality_node"); return state
def validate_model_node(state: dict) -> dict: print("-> EXECUTING: validate_model_node"); return state
def check_validation_status(state: dict) -> str: print("...CONDITION: check_validation_status -> success"); return "success"


NODE_LIBRARY = {
    "load_data": load_data_node,
    "discover_causality": discover_causality_node,
    "validate_model": validate_model_node,
    "check_validation_status": check_validation_status
}


--- FILE: dynamic_graph/outputsModels.py ---

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Type


class AgentSelection(BaseModel):
    """Struktura odpowiedzi dla Agenta-Rutera."""
    thought_process: str = Field(description="Krótki proces myślowy, dlaczego wybrano tych konkretnych agentów do zadania.")
    planners: List[str] = Field(description="Lista nazw agentów-planistów wybranych do zadania.")
    critic: str = Field(description="Nazwa agenta-krytyka wybranego do zadania.")

class NodeDefinition(BaseModel):
    name: str = Field(description="Unikalna nazwa węzła w grafie.")
    implementation: str = Field(description="Nazwa funkcji z biblioteki narzędzi, która realizuje ten węzeł.")

class EdgeDefinition(BaseModel):
    source: str = Field(alias="from", description="Nazwa węzła źródłowego.")
    target: Optional[str] = Field(alias="to", default=None, description="Nazwa węzła docelowego dla prostych krawędzi.")
    condition: Optional[str] = Field(default=None, description="Nazwa funkcji warunkowej z biblioteki narzędzi.")
    routes: Optional[Dict[str, str]] = Field(default=None, description="Mapa wyników warunku na nazwy węzłów docelowych.")

class WorkflowPlan(BaseModel):
    """Struktura odpowiedzi dla Agenta-Architekta. To jest finalny plan grafu."""
    thought_process: str = Field(description="Proces myślowy krok-po-kroku, który doprowadził do zaprojektowania tego konkretnego grafu.")
    entry_point: str = Field(description="Nazwa węzła, od którego zaczyna się przepływ pracy.")
    nodes: List[NodeDefinition]
    edges: List[EdgeDefinition]



--- FILE: dynamic_graph/router.py ---

import autogen
from typing import Optional, Dict, Any,Type
from pydantic import BaseModel, ValidationError
import json
from prompts import *

def get_structured_response(
    prompt: str, 
    response_model: Type[BaseModel], 
    llm_config: Dict, 
    max_retries: int = 3
) -> Optional[BaseModel]:
    """
    Wywołuje dowolnego agenta LLM i próbuje sparsować jego odpowiedź do modelu Pydantic.
    W przypadku błędu, prosi agenta o autokorektę.
    """
    # Używamy prostego agenta jako "wykonawcy" dla dowolnej konfiguracji LLM
    executor_agent = autogen.ConversableAgent("Executor", llm_config=llm_config)
    
    current_prompt = prompt
    for attempt in range(max_retries):
        print(f"INFO [StrucRes]: Próba {attempt + 1}/{max_retries}...")
        
        reply_dict  = executor_agent.generate_reply(messages=[{"role": "user", "content": current_prompt}])
        reply_content = reply_dict.get("content", "")
        try:
            # Próbujemy znaleźć i sparsować JSON z odpowiedzi
            json_str =reply_content[reply_content.find('{'):reply_content.rfind('}')+1]
            if not json_str:
                raise json.JSONDecodeError("Nie znaleziono obiektu JSON w odpowiedzi.", reply_content, 0)
            json_obj = json.loads(json_str)
            
            # Walidujemy, czy pasuje do naszego modelu Pydantic
            validated_obj = response_model.model_validate(json_obj)
            print("INFO [StrucRes]: Odpowiedź poprawna i zwalidowana.")
            return validated_obj
            
        except (json.JSONDecodeError, ValidationError) as e:
            print(f"OSTRZEŻENIE [StrucRes]: Odpowiedź LLM nie jest poprawnym obiektem. Błąd: {e}")
            # Tworzymy nowy prompt z prośbą o autokorektę
            current_prompt = f"{prompt}\n\nTwoja poprzednia odpowiedź była niepoprawna: '{reply}'. Proszę, popraw ją i zwróć TYLKO I WYŁĄCZNIE poprawny obiekt JSON."
    
    print("BŁĄD [StrucRes]: Nie udało się uzyskać poprawnej odpowiedzi po maksymalnej liczbie prób.")
    return None



def select_team(user_query: str, agent_library: Dict, router_llm_config: Dict) -> Dict[str, Any]:
    """Dynamicznie wybiera zespół, używając uniwersalnej funkcji do odpowiedzi strukturalnych."""
    print("--- RUTER AGENTÓW: Uruchamiam uniwersalny wybór zespołu... ---")
    
    agent_descriptions = "\n".join(
        [f"- {name}: {agent.system_message.split('Specjalizacja:')[1].strip()}" for name, agent in agent_library.items()]
    )
    # Tworzymy konfigurację i budujemy prompt tak jak poprzednio
    router_config = PromptEngine.for_router(user_query, agent_descriptions)
    router_prompt = PromptEngine.build(router_config)
    
    # NOWOŚĆ: Wywołujemy naszą uniwersalną funkcję
    selection_obj = get_structured_response(
        prompt=router_prompt,
        response_model=AgentSelection,
        llm_config=router_llm_config
    )
    
    if selection_obj:
        selection = selection_obj.model_dump()
        print(f"--- RUTER AGENTÓW: Wybrany zespół -> {selection} ---")
        return {
            "planners": [agent_library[name] for name in selection["planners"]],
            "critic": agent_library[selection["critic"]]
        }
    else:
        # Tryb awaryjny, jeśli LLM nie zwrócił poprawnej odpowiedzi
        print("--- RUTER AGENTÓW: Działam w trybie awaryjnym. Wybieram domyślny zespół. ---")
        return {
            "planners": [agent_library["Analityk_Danych"]],
            "critic": agent_library["Krytyk_Jakosci"]
        }



--- FILE: agents/agents.py ---

import autogen
from autogen import Agent, ConversableAgent


class CasualAgent(ConversableAgent):
    """Agent decydujący, czy dane nadają się do dalszego przetwarzania."""
    def __init__(self, llm_config, prompt):
        super().__init__(
            name="CasualAgent",
            llm_config=llm_config,
            system_message=prompt
        )

#PLANNER AGENT        
class DataScientistAgent(ConversableAgent):
    """Agent tworzący szczegółowy plan przygotowania danych."""
    def __init__(self, llm_config, prompt):
        super().__init__(
            name="DataScientistAgent",
            llm_config=llm_config,
            system_message=prompt
        )

#CRITIC AGENT
class CriticAgent(ConversableAgent):
    """Agent oceniający plan i dbający o jego jakość."""
    def __init__(self, llm_config, prompt):
        super().__init__(
            name="CriticAgent",
            llm_config=llm_config,
            system_message=prompt
        )


--- FILE: agents/agents_library.py ---

from prompts import *
from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET, ANTHROPIC_API_KEY,basic_config_agent
from .agents import *
# --- Konfiguracja czatu grupowego ---
main_agent_configuration={"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}
critic_agent_configuration ={"cache_seed": 42,"seed": 42,"temperature": 0.0,
                        "config_list": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}



casual_agent_config = PromptEngine.for_causal_expert(mission, node_descriptions)
casual_prompt = PromptEngine.build(casual_agent_config)



data_scientist_config = PromptEngine.for_data_analyst(mission, node_descriptions)
data_scientist_prompt = PromptEngine.build(data_scientist_config)


#---WYWOŁANIE AGENTÓW
casual_agent = CasualAgent(llm_config=main_agent_configuration, prompt=casual_prompt)

data_scientist_agent = DataScientistAgent(llm_config=main_agent_configuration, prompt=data_scientist_prompt)
critic_agent = CriticAgent(llm_config=critic_agent_configuration, prompt=critic_prompt)



AGENT_LIBRARY = {
    "Ekspert_Przyczynowosci": casual_agent,
    "Analityk_Danych": data_scientist_agent,
    "Krytyk_Jakosci": critic_agent,
}


